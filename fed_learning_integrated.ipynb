{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57_xKz1-Mpjg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import os\n",
        "import random\n",
        "\n",
        "# For reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "learning_rates = [0.005, 0.004, 0.003, 0.002, 0.001]\n",
        "batch_size = 64\n",
        "\n",
        "epochs = 5 #Number of epochs\n",
        "num_clients = 10  # Number of clients in federated learning\n",
        "federated_rounds = 3  # Number of rounds of federated training\n",
        "selection_factor = 0.5 # Number of clients to be selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gR7HnZKMpjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa89492-7e01-40e6-c93c-ca20ddf4373e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 18.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 312kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.51MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 6.47MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Load the training dataset\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "# Load the testing dataset\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YtKM17tMpjl"
      },
      "outputs": [],
      "source": [
        "# Convert the datasets to numpy arrays for further processing\n",
        "def convert_to_numpy(dataset):\n",
        "    X = []\n",
        "    y = []\n",
        "    for img, label in dataset:\n",
        "        X.append(img.cpu().numpy().flatten())  # Flatten the 28x28 image to a 1D array of 784 elements\n",
        "        y.append(label)\n",
        "    return np.array(X), np.array(y)\n",
        "    # return torch.tensor(np.array(X)),torch.tensor(np.array(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnA2jPN-Mpjm"
      },
      "outputs": [],
      "source": [
        "def format_data(train_dataset,test_dataset):# Convert train and test datasets to numpy arrays\n",
        "    X_train, y_train = convert_to_numpy(train_dataset)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    X_test, y_test = convert_to_numpy(test_dataset)\n",
        "\n",
        "    return X_train, X_val, y_train, y_val,X_test, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X-_rOTsgNTf"
      },
      "outputs": [],
      "source": [
        "def dataset_split(dataset):\n",
        "    X, y = convert_to_numpy(dataset)\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6uAXmErMpjm",
        "outputId": "849bd3e7-e4d3-4ec2-b562-5d59cdd7d1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Shape: (48000, 784), Train Labels Shape: (48000,)\n",
            "Validation Data Shape: (12000, 784), Validation Labels Shape: (12000,)\n",
            "Test Data Shape: (10000, 784), Test Labels Shape: (10000,)\n",
            "[9 2 1 1 6 1 4 6 5 7]\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, y_train, y_val,X_test, y_test = format_data(train_dataset,test_dataset)\n",
        "\n",
        "print(f'Train Data Shape: {X_train.shape}, Train Labels Shape: {y_train.shape}')\n",
        "print(f'Validation Data Shape: {X_val.shape}, Validation Labels Shape: {y_val.shape}')\n",
        "print(f'Test Data Shape: {X_test.shape}, Test Labels Shape: {y_test.shape}')\n",
        "print(y_test[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng1l1s4LMpjo"
      },
      "outputs": [],
      "source": [
        "class Dense:\n",
        "    def __init__(self, input_size, output_size, lr=0.005, beta1=0.9, beta2=0.999, epsilon=1e-8, device='cpu'):\n",
        "        self.device = device\n",
        "        self.weights = torch.randn(input_size, output_size, device=device) * np.sqrt(2.0 / input_size)\n",
        "        self.bias = torch.zeros(1, output_size, device=device)\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m_w, self.v_w = torch.zeros_like(self.weights), torch.zeros_like(self.weights)\n",
        "        self.m_b, self.v_b = torch.zeros_like(self.bias), torch.zeros_like(self.bias)\n",
        "        self.t = 0  # Time step\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        return torch.mm(X, self.weights) + self.bias  # Matrix multiplication (for GPU support)\n",
        "\n",
        "    def backward(self, d_output):\n",
        "        prev_weights = self.weights\n",
        "        d_weights = torch.mm(self.input.T, d_output)\n",
        "        d_bias = torch.sum(d_output, axis=0, keepdims=True)\n",
        "\n",
        "        # Update Adam time step\n",
        "        self.t += 1\n",
        "\n",
        "        # Moving averages of gradients for weights\n",
        "        self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * d_weights\n",
        "        self.v_w = self.beta2 * self.v_w + (1 - self.beta2) * (d_weights ** 2)\n",
        "\n",
        "        # Moving averages of gradients for biases\n",
        "        self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * d_bias\n",
        "        self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * (d_bias ** 2)\n",
        "\n",
        "        # Bias correction\n",
        "        m_w_hat = self.m_w / (1 - self.beta1 ** self.t)\n",
        "        v_w_hat = self.v_w / (1 - self.beta2 ** self.t)\n",
        "        m_b_hat = self.m_b / (1 - self.beta1 ** self.t)\n",
        "        v_b_hat = self.v_b / (1 - self.beta2 ** self.t)\n",
        "\n",
        "        self.weights -= self.lr * m_w_hat / (torch.sqrt(v_w_hat) + self.epsilon)\n",
        "        self.bias -= self.lr * m_b_hat / (torch.sqrt(v_b_hat) + self.epsilon)\n",
        "\n",
        "        # Return the gradient for the previous layer\n",
        "        return torch.mm(d_output, prev_weights.T)\n",
        "\n",
        "\n",
        "class ReLU:\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        return torch.maximum(torch.tensor(0.0, device=X.device), X)\n",
        "\n",
        "    def backward(self, d_output):\n",
        "        return d_output * (self.input > 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-tSzJPSMpjp"
      },
      "outputs": [],
      "source": [
        "class Dropout:\n",
        "    def __init__(self, rate=0.2):\n",
        "        self.rate = rate\n",
        "\n",
        "    def forward(self, X, training=True):\n",
        "        if training:\n",
        "            self.mask = (torch.rand(*X.shape, device=X.device) > self.rate) / (1 - self.rate)\n",
        "            return X * self.mask\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def backward(self, d_output):\n",
        "        return d_output * self.mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR-Pf8vbMpjq"
      },
      "outputs": [],
      "source": [
        "class BatchNorm:\n",
        "    def __init__(self, input_dim, lr=0.005, epsilon=1e-5, momentum=0.9, device='cpu'):\n",
        "        self.device = device\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "        self.gamma = torch.ones((1, input_dim), device=device)\n",
        "        self.beta = torch.zeros((1, input_dim), device=device)\n",
        "        self.running_mean = torch.zeros((1, input_dim), device=device)\n",
        "        self.running_var = torch.ones((1, input_dim), device=device)\n",
        "\n",
        "    def forward(self, X, training=True):\n",
        "        if training:\n",
        "            batch_mean = torch.mean(X, axis=0, keepdims=True)\n",
        "            batch_var = torch.var(X, axis=0, keepdims=True)\n",
        "\n",
        "            self.X_centered = X - batch_mean\n",
        "            self.stddev_inv = 1. / torch.sqrt(batch_var + self.epsilon)\n",
        "            X_norm = self.X_centered * self.stddev_inv\n",
        "\n",
        "            out = self.gamma * X_norm + self.beta  # to give the model flexibility to cancel out normalization\n",
        "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
        "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
        "        else:\n",
        "            X_norm = (X - self.running_mean) / torch.sqrt(self.running_var + self.epsilon)\n",
        "            out = self.gamma * X_norm + self.beta\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, d_output):\n",
        "        n_samples = d_output.shape[0]\n",
        "\n",
        "        d_gamma = torch.sum(d_output * self.X_centered * self.stddev_inv, axis=0, keepdims=True)\n",
        "        d_beta = torch.sum(d_output, axis=0, keepdims=True)\n",
        "\n",
        "        d_X_norm = d_output * self.gamma\n",
        "        d_var = torch.sum(d_X_norm * self.X_centered * -0.5 * self.stddev_inv**3, axis=0)\n",
        "        d_mean = torch.sum(d_X_norm * -self.stddev_inv, axis=0) + d_var * torch.mean(-2. * self.X_centered, axis=0)\n",
        "\n",
        "        d_input = d_X_norm * self.stddev_inv + d_var * 2 * self.X_centered / n_samples + d_mean / n_samples\n",
        "        self.gamma -= self.lr * d_gamma\n",
        "        self.beta -= self.lr * d_beta\n",
        "\n",
        "        return d_input\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqORsyjDMpjq"
      },
      "outputs": [],
      "source": [
        "class Softmax:\n",
        "    def forward(self, x):\n",
        "        x_max = torch.max(x, axis=1, keepdims=True).values\n",
        "        exp_values = torch.exp(x - x_max)\n",
        "        sum_exp_values = torch.sum(exp_values, axis=1, keepdims=True)\n",
        "        epsilon = 1e-10\n",
        "        probabilities = exp_values / (sum_exp_values + epsilon)\n",
        "        return probabilities\n",
        "\n",
        "    def backward(self, output, y):\n",
        "        n_samples = y.shape[0]\n",
        "        grad = output.clone()\n",
        "        grad[range(n_samples), y] -= 1\n",
        "        grad = grad / n_samples\n",
        "        return grad\n",
        "\n",
        "def cross_entropy_loss(predictions, labels):\n",
        "    n_samples = labels.shape[0]\n",
        "    logp = -torch.log(predictions[range(n_samples), labels] + 1e-15)\n",
        "    loss = torch.sum(logp) / n_samples\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5TyerARMpjr"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_dim, num_classes, lr=0.001, device='cpu'):\n",
        "        self.input_dim = input_dim\n",
        "        self.num_classes= num_classes\n",
        "        self.device = device\n",
        "        self.softmax = Softmax()\n",
        "        self.layers = [\n",
        "            Dense(input_dim, 128, lr, device=device),\n",
        "            BatchNorm(128, device=device),\n",
        "            ReLU(),\n",
        "            Dropout(),\n",
        "            Dense(128, 64, lr, device=device),\n",
        "            BatchNorm(64, device=device),\n",
        "            ReLU(),\n",
        "            Dropout(),\n",
        "            Dense(64, num_classes, lr, device=device)\n",
        "        ]\n",
        "\n",
        "    def forward(self, X, training=True):\n",
        "        X = X.to(self.device)\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, (BatchNorm, Dropout)):\n",
        "                X = layer.forward(X, training=training)\n",
        "            else:\n",
        "                X = layer.forward(X)\n",
        "\n",
        "        return self.softmax.forward(X)\n",
        "\n",
        "    def backward(self, d_output):\n",
        "        for layer in reversed(self.layers):\n",
        "            d_output = layer.backward(d_output)\n",
        "\n",
        "    #def train(self, X_train, y_train, X_val, y_val, epochs, batch_size):\n",
        "\n",
        "    def train(self, X_train, y_train, epochs, batch_size):\n",
        "\n",
        "        n_samples = X_train.shape[0]\n",
        "        training_loss, val_loss, training_acc, val_acc, val_f1 = [], [], [], [], []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            losses = []\n",
        "            for i in tqdm(range(0, n_samples, batch_size)):\n",
        "                x_batch = torch.tensor(X_train[i:i+batch_size], dtype=torch.float32).to(self.device)\n",
        "                y_batch = torch.tensor(y_train[i:i+batch_size], dtype=torch.long).to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                output = self.forward(x_batch, training=True)\n",
        "\n",
        "                # Loss calculation\n",
        "                loss = cross_entropy_loss(output, y_batch)\n",
        "                losses.append(loss)\n",
        "\n",
        "                # Backward pass\n",
        "                d_output = self.softmax.backward(output, y_batch)\n",
        "                self.backward(d_output)\n",
        "\n",
        "            # Training Loss & Accuracy\n",
        "            train_preds = self.predict(X_train)\n",
        "            train_accuracy = accuracy_score(y_train, train_preds)\n",
        "            losses = [loss.item() if isinstance(loss, torch.Tensor) else loss for loss in losses]\n",
        "            training_loss.append(np.mean(losses))\n",
        "            # training_loss.append(np.mean(losses.cpu().numpy()))\n",
        "            training_acc.append(train_accuracy)\n",
        "\n",
        "            # Validation Loss & Accuracy\n",
        "            # val_preds = self.predict(X_val)\n",
        "            # val_accuracy = accuracy_score(y_val, val_preds)\n",
        "            # val_f1_score = f1_score(y_val, val_preds, average=\"macro\")\n",
        "            # val_loss_epoch = cross_entropy_loss(self.forward(torch.tensor(X_val, dtype=torch.float32).to(self.device), training=False), torch.tensor(y_val, dtype=torch.long).to(self.device))\n",
        "\n",
        "            # val_loss.append(val_loss_epoch.item())\n",
        "            # val_acc.append(val_accuracy)\n",
        "            # val_f1.append(val_f1_score)\n",
        "\n",
        "            # print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {training_loss[-1]:.4f}, \"\n",
        "            #       f\"Val Loss: {val_loss[-1]:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
        "            #       f\"Val Acc: {val_accuracy:.4f}, Val F1: {val_f1_score:.4f}\")\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {training_loss[-1]:.4f}, \"\n",
        "                  f\"Train Acc: {train_accuracy:.4f}, \")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # return training_loss, val_loss, training_acc, val_acc, val_f1\n",
        "        return training_loss, training_acc\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(self.device)\n",
        "        output = self.forward(X_test_tensor, training=False)\n",
        "        predictions = torch.argmax(output, axis=1).cpu().numpy()  # Move to CPU for numpy conversion\n",
        "        return predictions\n",
        "\n",
        "    def evaluate(self, validation_dataset, test_dataset):\n",
        "\n",
        "        X_val, y_val = dataset_split(validation_dataset)\n",
        "        val_preds = self.predict(X_val)\n",
        "        val_accuracy = accuracy_score(y_val, val_preds)\n",
        "        #val_f1_score = f1_score(y_val, val_preds, average=\"macro\")\n",
        "        # val_loss_epoch = cross_entropy_loss(self.forward(torch.tensor(X_val, dtype=torch.float32).to(self.device), training=False), torch.tensor(y_val, dtype=torch.long).to(self.device))\n",
        "\n",
        "        # val_loss.append(val_loss_epoch.item())\n",
        "        # val_acc.append(val_accuracy)\n",
        "        # val_f1.append(val_f1_score)\n",
        "\n",
        "        X_test, y_test = dataset_split(test_dataset)\n",
        "\n",
        "        test_preds = self.predict(X_test)\n",
        "        test_accuracy = accuracy_score(y_test, test_preds)\n",
        "\n",
        "        print( f\"Val Acc: {val_accuracy:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCIdcD0pMpjs"
      },
      "outputs": [],
      "source": [
        "def split_data(X, y, num_clients):\n",
        "    client_data = []\n",
        "    client_labels = []\n",
        "    for i in range(num_clients):\n",
        "        # Assign data to each client\n",
        "        start = i * len(X) // num_clients\n",
        "        end = (i + 1) * len(X) // num_clients\n",
        "        client_data.append(X[start:end])\n",
        "        client_labels.append(y[start:end])\n",
        "    return client_data, client_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDch4qXeMpjs"
      },
      "outputs": [],
      "source": [
        "client_data, client_labels = split_data(X_train, y_train, num_clients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnVdvB4-Mpjt"
      },
      "outputs": [],
      "source": [
        "class Krum:\n",
        "\n",
        "    def __init__(self, localUpdates, n, f=1):\n",
        "        self.f = f\n",
        "        self.n = n\n",
        "        self.localUpdates = localUpdates\n",
        "        self.distanceSet = []\n",
        "        self.krum_scores = []\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def compute_Euclidian_Distances(self):\n",
        "\n",
        "        for i in range(self.n):\n",
        "            for key in self.localUpdates[i].keys():\n",
        "                self.localUpdates[i][key] = self.localUpdates[i][key].to(self.device)\n",
        "\n",
        "\n",
        "        stacked_updates = {key: torch.stack([self.localUpdates[i][key] for i in range(self.n)], dim=0) for key in self.localUpdates[0].keys()}\n",
        "\n",
        "\n",
        "        for i in range(self.n):\n",
        "\n",
        "            clientwise_Distance = []\n",
        "\n",
        "            for j in range(self.n):\n",
        "\n",
        "                distances = []\n",
        "\n",
        "                if i != j:\n",
        "\n",
        "\n",
        "                    for key in stacked_updates.keys():\n",
        "\n",
        "                        diff = stacked_updates[key][i] - stacked_updates[key][j]\n",
        "                        distances.append((diff ** 2).sum())\n",
        "\n",
        "\n",
        "                    total_distance = torch.sqrt(torch.sum(torch.stack(distances)))\n",
        "\n",
        "                    clientwise_Distance.append(total_distance)\n",
        "\n",
        "            self.distanceSet.append(clientwise_Distance)\n",
        "\n",
        "    def krum_aggregation(self):\n",
        "\n",
        "        self.compute_Euclidian_Distances()\n",
        "\n",
        "        distanceSet_tensor = torch.tensor(self.distanceSet, device=self.device)\n",
        "\n",
        "        krum_scores = []\n",
        "\n",
        "        for i in range(self.n):\n",
        "\n",
        "            distances = distanceSet_tensor[i]\n",
        "\n",
        "            sorted_distances,_ = torch.sort(distances)\n",
        "\n",
        "\n",
        "            top_distances = sorted_distances[:self.n - self.f - 1]\n",
        "\n",
        "            krum_score = top_distances.sum()\n",
        "\n",
        "            krum_scores.append(krum_score)\n",
        "\n",
        "        krum_scores_tensor = torch.tensor(krum_scores, device=self.device)\n",
        "\n",
        "\n",
        "        best_client_index = torch.argmin(krum_scores_tensor).item()\n",
        "\n",
        "\n",
        "        return best_client_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pIRKCsXMpjt"
      },
      "outputs": [],
      "source": [
        "class Server:\n",
        "\n",
        "    def __init__(self ,model, validation_dataset, test_dataset, learning_rate = 0.01, aggregation_method = \"FedAvg\", device= \"cuda\"):\n",
        "        self.global_model = model\n",
        "        self.clientList = [] #List of all clients registered\n",
        "        self.selectedClientList = [] #List of clients selected in a specific round\n",
        "        self.markedClientList = [] #List of clients marked vulnerable( Not yet used )\n",
        "        self.learning_rate = learning_rate\n",
        "        self.device = device\n",
        "        self.aggregation_method = aggregation_method\n",
        "        self.validation_dataset = validation_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "\n",
        "    # Register Clients\n",
        "\n",
        "    def register(self, clients):\n",
        "        self.clientList.append(clients)\n",
        "        print(\"Registering client\")\n",
        "\n",
        "\n",
        "    # Select Clients in each round\n",
        "\n",
        "    def select(self, clientMap):\n",
        "\n",
        "        client_id, client = clientMap\n",
        "        self.selectedClientList.append(clientMap)\n",
        "        print(\"Selecting client \", client_id)\n",
        "\n",
        "    # Reset the server in each round\n",
        "\n",
        "    def reset(self):\n",
        "        self.selectedClientList = []\n",
        "\n",
        "    # Aggregrate updates\n",
        "\n",
        "    def aggregate(self, localUpdates):\n",
        "\n",
        "        aggregated_params = {}\n",
        "\n",
        "        print(\"Getting aggregated by server\")\n",
        "\n",
        "        if self.aggregation_method == 'FedAvg':\n",
        "\n",
        "\n",
        "            for key in localUpdates[0].keys():\n",
        "                aggregated_params[key] = torch.stack([torch.tensor(update[key].float(),device=self.device) for update in localUpdates], dim=0).mean(dim=0)\n",
        "            return aggregated_params\n",
        "\n",
        "        if self.aggregation_method == 'Krum':\n",
        "\n",
        "            krum = Krum(localUpdates, len(self.selectedClientList))\n",
        "            best_client_index = krum.krum_aggregation()\n",
        "\n",
        "            print(f\"Best client for aggregation: {self.selectedClientList[best_client_index][0]}\")\n",
        "            return localUpdates[best_client_index]\n",
        "\n",
        "\n",
        "    # Update global model with aggregrated parameters\n",
        "\n",
        "\n",
        "    def update_global_model(self, localUpdates):\n",
        "\n",
        "        aggregated_params = self.aggregate(localUpdates )\n",
        "\n",
        "        # Layer by layer update weight to model self.global_model.\n",
        "        for i, layer in enumerate(self.global_model.layers):\n",
        "            if isinstance(layer, Dense):\n",
        "                layer.weights = aggregated_params[f\"layer_{i}_weights\"]\n",
        "                layer.bias = aggregated_params[f\"layer_{i}_bias\"]\n",
        "            elif isinstance(layer, BatchNorm):\n",
        "                layer.gamma = aggregated_params[f\"layer_{i}_gamma\"]\n",
        "                layer.beta = aggregated_params[f\"layer_{i}_beta\"]\n",
        "                layer.running_mean = aggregated_params[f\"layer_{i}_running_mean\"]\n",
        "                layer.running_mean = aggregated_params[f\"layer_{i}_running_var\"]\n",
        "\n",
        "\n",
        "        print(\"Done Aggregating\")\n",
        "\n",
        "    def evaluate(self):\n",
        "      self.global_model.evaluate(self.validation_dataset, self.test_dataset)\n",
        "\n",
        "    # Distribute updated model to clients\n",
        "\n",
        "    def distribute_global_model(self):\n",
        "        for client in self.clientList:\n",
        "            client.receive_model(self.global_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWEWZdMUMpju"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Federated Learning Client Class\n",
        "class Client:\n",
        "    def __init__(self, model, client_train_data, epochs, batch_size, device='cpu'):\n",
        "        self.client_train_data=client_train_data\n",
        "        #self.client_test_data=client_test_data\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        X_train, y_train = dataset_split(self.client_train_data)\n",
        "         # print(f'Train Data Shape: {X_train.shape}, Train Labels Shape: {y_train.shape}')\n",
        "            # print(f'Train Data Shape: {X_val.shape}, Train Labels Shape: {y_val.shape}')\n",
        "            # print(f'Test Data Shape: {X_test.shape}, Test Labels Shape: {y_test.shape}')\n",
        "            # print(y_test[:10])\n",
        "\n",
        "        # train_loss, val_loss, train_acc, val_acc, val_f1 = self.model.train(\n",
        "        #     X_train, y_train, X_val, y_val, epochs, batch_size\n",
        "        # )\n",
        "\n",
        "        # return train_loss, val_loss, train_acc, val_acc, val_f1\n",
        "\n",
        "        #train_loss, train_acc = self.model.train( X_train, y_train, X_val, y_val, epochs, batch_size )\n",
        "\n",
        "        train_loss, train_acc = self.model.train( X_train, y_train, epochs, batch_size )\n",
        "\n",
        "    def get_parameters(self):\n",
        "        weights = {}\n",
        "        for i, layer in enumerate(self.model.layers):\n",
        "            if isinstance(layer, Dense):\n",
        "                weights[f\"layer_{i}_weights\"] = layer.weights\n",
        "                weights[f\"layer_{i}_bias\"] = layer.bias\n",
        "            elif isinstance(layer, BatchNorm):\n",
        "                weights[f\"layer_{i}_gamma\"] = layer.gamma\n",
        "                weights[f\"layer_{i}_beta\"] = layer.beta\n",
        "                weights[f\"layer_{i}_running_mean\"] = layer.running_mean\n",
        "                weights[f\"layer_{i}_running_var\"] = layer.running_var\n",
        "\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def receive_model(self,global_model):\n",
        "        self.model=global_model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmzFADT3Mpju",
        "outputId": "6e7f2704-5de5-4ead-8ba0-106fb3c2af7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registering client\n",
            "Registering client\n",
            "Registering client\n",
            "Registering client\n",
            "Registering client\n",
            "Registering client\n",
            "Registering client\n",
            "Registering client\n",
            "Registering client\n",
            "Registering client\n",
            "Selecting client  1\n",
            "Selecting client  2\n",
            "Selecting client  5\n",
            "Selecting client  6\n",
            "Selecting client  8\n",
            "Training started by client  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 155.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.8246, Train Acc: 0.8208, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 188.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.5665, Train Acc: 0.8320, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 205.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.4910, Train Acc: 0.8618, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 185.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.4457, Train Acc: 0.8453, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 198.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.4198, Train Acc: 0.8723, \n",
            "Training completed by client  1\n",
            "Training started by client  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 143.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.4979, Train Acc: 0.8583, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 142.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.4302, Train Acc: 0.8737, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 131.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.3891, Train Acc: 0.8847, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 132.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.3595, Train Acc: 0.8958, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 128.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.3295, Train Acc: 0.9013, \n",
            "Training completed by client  2\n",
            "Training started by client  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 201.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.4607, Train Acc: 0.8773, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 190.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.4059, Train Acc: 0.8818, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 199.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.3638, Train Acc: 0.8970, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 190.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.3371, Train Acc: 0.9025, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 206.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.3142, Train Acc: 0.9102, \n",
            "Training completed by client  5\n",
            "Training started by client  6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 197.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.4561, Train Acc: 0.8795, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 197.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.3868, Train Acc: 0.8912, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 186.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.3354, Train Acc: 0.9010, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 198.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.3147, Train Acc: 0.9003, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 194.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.3041, Train Acc: 0.9043, \n",
            "Training completed by client  6\n",
            "Training started by client  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 189.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.4245, Train Acc: 0.8785, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 188.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.3518, Train Acc: 0.8962, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 187.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.3316, Train Acc: 0.8892, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 181.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.3053, Train Acc: 0.9047, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 134.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.2806, Train Acc: 0.9127, \n",
            "Training completed by client  8\n",
            "Getting aggregated by server\n",
            "Best client for aggregation: 5\n",
            "Done Aggregating\n",
            "Val Acc: 0.1000, Test Acc: 0.1000\n",
            "Selecting client  0\n",
            "Selecting client  5\n",
            "Selecting client  8\n",
            "Selecting client  9\n",
            "Training started by client  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 161.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.4500, Train Acc: 0.8770, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 140.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.3599, Train Acc: 0.8932, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 129.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.3192, Train Acc: 0.9015, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 115.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.3129, Train Acc: 0.9033, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 110.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.2947, Train Acc: 0.9173, \n",
            "Training completed by client  0\n",
            "Training started by client  5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 122.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.3762, Train Acc: 0.9010, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 108.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.3110, Train Acc: 0.9123, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 154.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.2821, Train Acc: 0.9195, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 157.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.2661, Train Acc: 0.9260, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 167.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.2489, Train Acc: 0.9253, \n",
            "Training completed by client  5\n",
            "Training started by client  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 174.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.3417, Train Acc: 0.9017, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 156.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.2945, Train Acc: 0.9143, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 170.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.2597, Train Acc: 0.9095, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 157.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.2416, Train Acc: 0.9252, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 157.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.2207, Train Acc: 0.9145, \n",
            "Training completed by client  8\n",
            "Training started by client  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 137.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.4593, Train Acc: 0.8828, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 97.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.3691, Train Acc: 0.8875, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 119.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.3435, Train Acc: 0.8972, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 135.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.3104, Train Acc: 0.9037, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 140.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.2864, Train Acc: 0.9083, \n",
            "Training completed by client  9\n",
            "Getting aggregated by server\n",
            "Best client for aggregation: 8\n",
            "Done Aggregating\n",
            "Val Acc: 0.1000, Test Acc: 0.1000\n",
            "Selecting client  0\n",
            "Selecting client  1\n",
            "Selecting client  4\n",
            "Selecting client  8\n",
            "Selecting client  9\n",
            "Training started by client  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 191.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.3428, Train Acc: 0.9087, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 189.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.2929, Train Acc: 0.9182, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 190.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.2693, Train Acc: 0.9268, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 178.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.2470, Train Acc: 0.9268, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 203.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.2314, Train Acc: 0.9240, \n",
            "Training completed by client  0\n",
            "Training started by client  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 196.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.4399, Train Acc: 0.8965, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 198.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.3551, Train Acc: 0.9035, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 191.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.3279, Train Acc: 0.9113, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 199.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.2972, Train Acc: 0.9177, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 190.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.2839, Train Acc: 0.9247, \n",
            "Training completed by client  1\n",
            "Training started by client  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 204.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.4261, Train Acc: 0.8872, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 194.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.3588, Train Acc: 0.9018, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 203.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.3247, Train Acc: 0.9053, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 195.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.2992, Train Acc: 0.9105, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 167.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.2824, Train Acc: 0.9210, \n",
            "Training completed by client  4\n",
            "Training started by client  8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 138.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.2994, Train Acc: 0.9215, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 122.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.2596, Train Acc: 0.9215, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 138.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.2301, Train Acc: 0.9283, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 113.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.2145, Train Acc: 0.9332, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:01<00:00, 88.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.2011, Train Acc: 0.9388, \n",
            "Training completed by client  8\n",
            "Training started by client  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:01<00:00, 60.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.3730, Train Acc: 0.9108, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:01<00:00, 70.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Train Loss: 0.3099, Train Acc: 0.9093, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 123.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Train Loss: 0.2828, Train Acc: 0.9200, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 193.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Train Loss: 0.2558, Train Acc: 0.9288, \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 94/94 [00:00<00:00, 202.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Train Loss: 0.2460, Train Acc: 0.9238, \n",
            "Training completed by client  9\n",
            "Getting aggregated by server\n",
            "Best client for aggregation: 4\n",
            "Done Aggregating\n",
            "Val Acc: 0.1000, Test Acc: 0.1000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Main Federated Learning Process\n",
        "def federated_learning_simulation(epochs, batch_size, learning_rates, num_clients, federated_rounds, selection_factor, train_dataset, test_dataset ):\n",
        "\n",
        "    # X_train, X_val, y_train, y_val, X_test, y_test = format_data(train_dataset,test_dataset)\n",
        "\n",
        "\n",
        "    # # Optionally, if you want to divide the test dataset as well (e.g., each client gets a portion of the test set)\n",
        "    #client_test_data = torch.utils.data.random_split(test_dataset, [len(test_dataset) // num_clients] * num_clients)\n",
        "\n",
        "    #client_train_data = torch.utils.data.random_split(train_dataset, [len(train_dataset) // num_clients] * num_clients)\n",
        "    # X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    # y_train_tensor = torch.tensor(y_train, dtype=torch.int)\n",
        "\n",
        "\n",
        "\n",
        "    # train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "\n",
        "    client_train_data = torch.utils.data.random_split(train_dataset, [len(train_dataset) // num_clients] * num_clients)\n",
        "\n",
        "    # X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "    # y_val_tensor = torch.tensor(y_val, dtype=torch.int)\n",
        "\n",
        "    # validation_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "    global_model = NeuralNetwork(input_dim=784, num_classes=10, lr=learning_rates[0], device=device)\n",
        "\n",
        "    server = Server(model=global_model, learning_rate = 0.01, aggregation_method = \"Krum\", validation_dataset = train_dataset, test_dataset = test_dataset, device=device)\n",
        "\n",
        "    for i in range(0,num_clients):\n",
        "        client = Client(global_model, client_train_data[i], epochs, batch_size, device)\n",
        "        server.register(client)\n",
        "\n",
        "\n",
        "    for round in range(federated_rounds):\n",
        "\n",
        "        server.reset()\n",
        "\n",
        "        for i in range(0,num_clients):\n",
        "\n",
        "            rd = random.randint(0,100)\n",
        "            if rd < selection_factor*100:\n",
        "                clientMap = i, server.clientList[i]\n",
        "                server.select( clientMap )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        client_weights=[]\n",
        "\n",
        "        for client_id,client in server.selectedClientList:\n",
        "\n",
        "            print(\"Training started by client \", client_id )\n",
        "\n",
        "            client.train()\n",
        "\n",
        "            client_weights.append(client.get_parameters())\n",
        "\n",
        "            print(\"Training completed by client \", client_id)\n",
        "\n",
        "        server.update_global_model(client_weights)\n",
        "        server.evaluate()\n",
        "        server.distribute_global_model()\n",
        "\n",
        "\n",
        "\n",
        "federated_learning_simulation(epochs, batch_size, learning_rates, num_clients, federated_rounds, selection_factor, train_dataset, test_dataset )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CihrK-qMpjv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# best_model = None\n",
        "# best_f1_score = 0\n",
        "\n",
        "# # Run experiments\n",
        "# results = {}\n",
        "# for lr in learning_rates:\n",
        "#     print(f\"Training with learning rate: {lr}\")\n",
        "\n",
        "#     # Initialize the model\n",
        "#     model = NeuralNetwork(input_dim=784, num_classes=10, lr=lr, device=device)\n",
        "\n",
        "#     # Train the model and get training and validation metrics\n",
        "#     train_loss, val_loss, train_acc, val_acc, val_f1 = model.train(\n",
        "#         X_train, y_train, X_val, y_val, epochs, batch_size\n",
        "#     )\n",
        "\n",
        "#     # Store the results\n",
        "#     results[lr] = (train_loss, val_loss, train_acc, val_acc, val_f1)\n",
        "\n",
        "#     # Track the best model based on validation F1 score\n",
        "#     if val_f1[-1] > best_f1_score:\n",
        "#         best_f1_score = val_f1[-1]\n",
        "#         print(f\"Best model found with learning rate: {lr} and F1 score: {best_f1_score}\")\n",
        "#         best_model = model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg6TeGjzMpjw"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Make predictions with the best model\n",
        "# Y_pred = best_model.predict(X_test)\n",
        "# accuracy = accuracy_score(y_test, Y_pred)\n",
        "# precision = precision_score(y_test, Y_pred, average=\"weighted\")\n",
        "# recall = recall_score(y_test, Y_pred, average=\"weighted\")\n",
        "# f1 = f1_score(y_test, Y_pred, average=\"weighted\")\n",
        "\n",
        "# # Print the evaluation metrics for the best model\n",
        "# print(f\"Accuracy: {accuracy}\")\n",
        "# print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}